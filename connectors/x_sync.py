import os
import json
import time
import requests
import re
from datetime import datetime
from dotenv import load_dotenv

# åŠ è½½é…ç½®
load_dotenv()
API_KEY = os.getenv("RAPIDAPI_KEY")
API_HOST = os.getenv("RAPIDAPI_HOST")

if not API_KEY or not API_HOST:
    print("âŒ é”™è¯¯: è¯·æ£€æŸ¥ .env æ–‡ä»¶ä¸­çš„ API Key å’Œ Host è®¾ç½®")
    exit()

# åŸºç¡€é…ç½®
BASE_URL = f"https://{API_HOST}"
HEADERS = {
    "X-RapidAPI-Key": API_KEY,
    "X-RapidAPI-Host": API_HOST,
    "Content-Type": "application/json"
}

# --- âš™ï¸ æŠ“å–è®¾ç½® ---
MAX_PAGES = 10     # æƒ³æŠ“å¤šå°‘é¡µï¼Ÿ(æ¯é¡µçº¦40æ¡)
TIME_SLEEP = 2     # ç¿»é¡µé—´éš”ç§’æ•° (é˜²å°)
STATE_PATH = os.path.join("state", "sync_state.json")
DATA_DIR = os.path.join("data_sources", "x")


def convert_to_markdown(username, json_path):
    """å°† JSON è½¬æ¢ä¸º Markdown"""
    print(f"âš™ï¸ æ­£åœ¨å°†æ•°æ®è½¬æ¢ä¸º Markdown...")
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            all_pages = json.load(f) # æ³¨æ„ï¼šè¿™é‡Œè¯»å…¥çš„æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ˆå¤šé¡µæ•°æ®ï¼‰
        
        # å‡†å¤‡ Markdown å¤´éƒ¨
        md = f"# Twitter Archive: @{username}\n\n"
        md += f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n---\n\n"

        total_tweets = 0
        
        # éå†æ¯ä¸€é¡µæ•°æ®
        for page_data in all_pages:
            tweets = []
            try:
                # é€‚é… Twttr API ç»“æ„: result -> timeline -> instructions
                instructions = page_data.get('result', {}).get('timeline', {}).get('instructions', [])
                for instr in instructions:
                    if instr.get('type') == 'TimelineAddEntries':
                        tweets = instr.get('entries', [])
                        break
            except:
                continue

            # éå†å•é¡µé‡Œçš„æ¨æ–‡
            for entry in tweets:
                if not entry.get('entryId', '').startswith('tweet-'): continue
                try:
                    res = entry['content']['itemContent']['tweet_results']['result']
                    legacy = res.get('legacy') or res
                    
                    text = legacy.get('full_text', '').replace('\n', '\n> ')
                    date = legacy.get('created_at', '')
                    tid = legacy.get('id_str', '')
                    
                    # å†™å…¥ Markdown
                    md += f"### ğŸ“… {date}\n\n> {text}\n\n"
                    md += f"ğŸ”— [Link](https://twitter.com/{username}/status/{tid})\n\n---\n\n"
                    total_tweets += 1
                except: continue

        # ä¿å­˜ Markdown
        md_path = json_path.replace('.json', '.md')
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(md)
        print(f"âœ¨ Markdown ç¬”è®°å·²ç”Ÿæˆ: {md_path} (å…± {total_tweets} æ¡)")
            
    except Exception as e:
        print(f"âŒ è½¬æ¢ Markdown å¤±è´¥: {e}")

def save_to_json(username, all_data):
    """ä¿å­˜æ‰€æœ‰é¡µçš„æ•°æ®"""
    output_dir = "data_sources"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    filename = os.path.join(output_dir, f"twitter_{username}_rapid.json")
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æ•°æ®å·²ä¿å­˜: {filename} (å…± {len(all_data)} é¡µ)")
    
    # è‡ªåŠ¨è½¬æ¢
    convert_to_markdown(username, filename)

def get_user_id(username):
    """è·å–ç”¨æˆ· ID (é€‚é… Twttr API)"""
    print(f"ğŸ” æ­£åœ¨æŸ¥è¯¢ @{username} çš„ ID...")
    url = f"{BASE_URL}/user" 
    params = {"username": username}

    try:
        response = requests.get(url, headers=HEADERS, params=params)
        data = response.json()
        
        # å°è¯•å¤šå±‚æå–
        try:
            return data.get("result", {}).get("data", {}).get("user", {}).get("result", {}).get("rest_id")
        except: pass
        
        if "rest_id" in data: return data["rest_id"]
        if "id" in data: return data["id"]
        
        print(f"âš ï¸ æœªæ‰¾åˆ° ID: {str(data)[:100]}...")
        return None
    except Exception as e:
        print(f"âŒ ID æŸ¥è¯¢å‡ºé”™: {e}")
        return None

def extract_cursor(data):
    """ä»ä¸€é¡µæ•°æ®ä¸­æå–ç¿»é¡µç”¨çš„ cursor"""
    # 1. å°è¯•ä»æ ‡å‡†ç»“æ„æå–
    try:
        instructions = data.get('result', {}).get('timeline', {}).get('instructions', [])
        for instr in instructions:
            if instr.get('type') == 'TimelineAddEntries':
                entries = instr.get('entries', [])
                for entry in entries:
                    if str(entry.get('entryId', '')).startswith('cursor-bottom-'):
                        return entry['content']['itemContent']['value']
    except: pass
    
    # 2. å¦‚æœç»“æ„å˜äº†ï¼Œç”¨æ­£åˆ™æš´åŠ›æå–
    data_str = json.dumps(data)
    # å¯»æ‰¾ value å­—æ®µä¸­ä»¥ DAA å¼€å¤´çš„é•¿å­—ç¬¦ä¸²
    matches = re.findall(r'"value"\s*:\s*"(DAA[^"]+)"', data_str)
    if matches:
        return matches[-1] # è¿”å›æœ€åä¸€ä¸ª cursor (é€šå¸¸æ˜¯ä¸‹ä¸€é¡µ)
        
    return None

def _load_state() -> dict:
    try:
        if os.path.exists(STATE_PATH):
            with open(STATE_PATH, "r", encoding="utf-8") as f:
                data = json.load(f)
                return data if isinstance(data, dict) else {}
    except Exception:
        pass
    return {}

def _save_state(state: dict) -> None:
    os.makedirs(os.path.dirname(STATE_PATH), exist_ok=True)
    with open(STATE_PATH, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)

def _get_x_users_state(state: dict) -> dict:
    state.setdefault("x_users", {})
    if not isinstance(state["x_users"], dict):
        state["x_users"] = {}
    return state["x_users"]

def _extract_tweets_from_page(page_data: dict) -> list:
    tweets = []
    try:
        instructions = page_data.get("result", {}).get("timeline", {}).get("instructions", [])
        entries = []
        for instr in instructions:
            if instr.get("type") == "TimelineAddEntries":
                entries = instr.get("entries", [])
                break

        for entry in entries:
            if not str(entry.get("entryId", "")).startswith("tweet-"):
                continue
            try:
                res = entry["content"]["itemContent"]["tweet_results"]["result"]
                legacy = res.get("legacy") or res
                tid = legacy.get("id_str") or ""
                if not tid:
                    continue
                tweets.append({
                    "id": str(tid),
                    "created_at": legacy.get("created_at", ""),
                    "text": (legacy.get("full_text", "") or "").strip(),
                })
            except Exception:
                continue
    except Exception:
        pass
    return tweets

def fetch_updates(username: str, max_pages: int = 2) -> int:
    """
    å¢é‡æŠ“å–ï¼šåªæŠ“â€œä¸Šæ¬¡æœ€æ–° tweet idâ€ä¹‹åçš„æ–°è´´æ–‡ã€‚
    è¿›åº¦å†™å…¥ state/sync_state.json -> x_users[username].latest_id
    æ–°å†…å®¹è½ç›˜åˆ° data_sources/x/<username>/tweets_<timestamp>.mdï¼ˆåˆ©äº ingest å¢é‡ï¼‰
    è¿”å›ï¼šæ–°å¢ tweet æ•°
    """
    username = (username or "").strip().lstrip("@")
    if not username:
        print("âš ï¸ [X] username ä¸ºç©ºï¼Œè·³è¿‡")
        return 0

    state = _load_state()
    x_users = _get_x_users_state(state)
    u = x_users.setdefault(username, {})

    user_id = u.get("user_id")
    if not user_id:
        user_id = get_user_id(username)
        if not user_id:
            print(f"âŒ [X] æ— æ³•è·å– @{username} çš„ user_id")
            return 0
        u["user_id"] = user_id

    last_seen_id = u.get("latest_id")
    print(f"ğŸ¦ [X] @{username} å¢é‡åŒæ­¥å¼€å§‹ (last_seen_id={last_seen_id})")

    url = f"{BASE_URL}/user-tweets"
    cursor = None
    raw_pages = []
    collected = []
    stop = False

    for _ in range(max_pages):
        params = {"user": user_id, "include_replies": "false", "count": 40}
        if cursor:
            params["cursor"] = cursor

        resp = requests.get(url, headers=HEADERS, params=params)
        if resp.status_code != 200:
            print(f"âŒ [X] è¯·æ±‚å¤±è´¥: {resp.status_code} {resp.text[:120]}")
            break

        data = resp.json()
        raw_pages.append(data)

        for t in _extract_tweets_from_page(data):
            if last_seen_id and t["id"] == last_seen_id:
                stop = True
                break
            collected.append(t)

        if stop:
            break

        next_cursor = extract_cursor(data)
        if not next_cursor or next_cursor == cursor:
            break
        cursor = next_cursor
        time.sleep(TIME_SLEEP)

    # å»é‡
    uniq = []
    seen = set()
    for t in collected:
        if t["id"] in seen:
            continue
        seen.add(t["id"])
        uniq.append(t)

    if not uniq:
        print(f"ğŸ’¤ [X] @{username} æ²¡æœ‰æ–°è´´æ–‡")
        return 0

    # æ›´æ–° latest_idï¼ˆå–æœ€å¤§ï¼‰
    try:
        latest_id = str(max(int(t["id"]) for t in uniq))
    except Exception:
        latest_id = uniq[0]["id"]

    u["latest_id"] = latest_id
    u["last_sync_at"] = datetime.now().isoformat(timespec="seconds")
    _save_state(state)

    # å†™æ–°å¢æ–‡ä»¶ï¼ˆæ¯æ¬¡ä¸€ä¸ªæ–° mdï¼‰
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_dir = os.path.join(DATA_DIR, username)
    os.makedirs(out_dir, exist_ok=True)

    md_path = os.path.join(out_dir, f"tweets_{ts}.md")
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(f"# X Incremental: @{username}\n\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"New tweets: {len(uniq)}\n\n---\n\n")
        for t in uniq:
            text = (t["text"] or "").replace("\n", "\n> ")
            f.write(f"### ğŸ“… {t['created_at']}\n\n> {text}\n\n")
            f.write(f"ğŸ”— [Link](https://twitter.com/{username}/status/{t['id']})\n\n---\n\n")

    raw_path = os.path.join(out_dir, f"raw_{ts}.json")
    with open(raw_path, "w", encoding="utf-8") as f:
        json.dump(raw_pages, f, ensure_ascii=False, indent=2)

    print(f"âœ… [X] @{username} æ–°å¢ {len(uniq)} æ¡ï¼Œå·²å†™å…¥: {md_path}")
    return len(uniq)

def fetch_all_tweets(username, user_id):
    """ä¸»æŠ“å–å¾ªç¯"""
    print(f"ğŸš€ å¼€å§‹æŠ“å–...")
    url = f"{BASE_URL}/user-tweets"
    
    all_pages = []
    cursor = None
    page = 0
    
    while page < MAX_PAGES:
        page += 1
        print(f"ğŸ“„ ç¬¬ {page} é¡µ...", end="", flush=True)
        
        params = {
            "user": user_id,
            "include_replies": "false",
            "count": 40
        }
        if cursor:
            params["cursor"] = cursor
            
        try:
            response = requests.get(url, headers=HEADERS, params=params)
            
            if response.status_code != 200:
                print(f" âŒ å¤±è´¥: {response.status_code}")
                break
                
            data = response.json()
            all_pages.append(data)
            print(" âœ…", end="")
            
            # æ‰¾ä¸‹ä¸€é¡µçš„ cursor
            next_cursor = extract_cursor(data)
            if next_cursor and next_cursor != cursor:
                cursor = next_cursor
                print(f" (æ‰¾åˆ°ä¸‹ä¸€é¡µ)")
                time.sleep(TIME_SLEEP)
            else:
                print(" (å·²åˆ°æœ«å°¾)")
                break
                
        except Exception as e:
            print(f"\nâŒ å‡ºé”™: {e}")
            break
            
    # å¾ªç¯ç»“æŸåä¿å­˜
    if all_pages:
        save_to_json(username, all_pages)
    else:
        print("âŒ æœªæŠ“å–åˆ°æ•°æ®")

if __name__ == "__main__":
    target_user = input("è¯·è¾“å…¥ç”¨æˆ·å: ").strip()
    if target_user:
        uid = get_user_id(target_user)
        if uid:
            print(f"âœ… ID: {uid}")
            fetch_all_tweets(target_user, uid)
        else:
            print("âŒ æ— æ³•è·å– ID")