import os
import json
import time
import random
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI

load_dotenv()

CORPUS_PATH = "outputs/corpus.jsonl"
PROFILE_PATH = "outputs/user_profile.md"
PROFILE_STATE = "state/profile_state.json"

def _is_overload_error(e: Exception) -> bool:
    msg = str(e)
    return ("503" in msg) or ("overloaded" in msg.lower()) or ("UNAVAILABLE" in msg)

def _retry(callable_fn, retries=6, base_delay=2.0, max_delay=30.0):
    """
    åªå¯¹ 503/overloaded åšé‡è¯•ï¼›å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡ºã€‚
    """
    for i in range(retries):
        try:
            return callable_fn()
        except Exception as e:
            if not _is_overload_error(e):
                raise
            sleep_s = min(max_delay, base_delay * (2 ** i) + random.random())
            print(f"âš ï¸ [LLM] 503/overloadedï¼Œç¬¬ {i+1}/{retries} æ¬¡é‡è¯•ï¼Œ{sleep_s:.1f}s åå†è¯•â€¦")
            time.sleep(sleep_s)
    raise RuntimeError("LLM 503/overloadedï¼šå¤šæ¬¡é‡è¯•ä»å¤±è´¥")

def _load_state():
    if not os.path.exists(PROFILE_STATE):
        return {"last_line": 0}
    with open(PROFILE_STATE, "r", encoding="utf-8") as f:
        return json.load(f)

def _save_state(state):
    with open(PROFILE_STATE, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)

def _read_new_chunks(max_items=40):
    # 1. å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨å’Œ0
    if not os.path.exists(CORPUS_PATH):
        return [], 0

    # 2. è¯»å–æ—§çš„çŠ¶æ€
    state = _load_state()
    last_line = int(state.get("last_line", 0))

    # 3. è¯»å–æ–‡ä»¶æ‰€æœ‰è¡Œ
    with open(CORPUS_PATH, "r", encoding="utf-8") as f:
        lines = f.readlines()

    # ã€å…³é”®ä¿®å¤ã€‘æ— è®ºæ˜¯å¦æœ‰æ–°å†…å®¹ï¼Œå…ˆç¡®å®šç°åœ¨çš„æ€»è¡Œæ•°
    new_last_line = len(lines)

    # 4. æˆªå–æ–°å¢åŠ çš„è¡Œ
    new_lines = lines[last_line:]

    chunks = []
    for ln in new_lines:
        try:
            obj = json.loads(ln)
            chunks.append(obj)
        except:
            pass

    # ã€å…³é”®ä¿®å¤ã€‘å¿…é¡»æŠŠ chunks å’Œ new_last_line éƒ½è¿”å›å‡ºå»
    return chunks, new_last_line

def update_user_profile():
    if not os.getenv("GOOGLE_API_KEY"):
        print("âŒ ç¼ºå°‘ GOOGLE_API_KEYï¼Œæ— æ³•æ›´æ–°ç”»åƒ")
        return False
        
    state = _load_state()
    chunks, new_last_line = _read_new_chunks()
    old_last_line = int(state.get("last_line", 0))
    raw_new_line_count = new_last_line - old_last_line

    if not chunks:
        print("ğŸ’¤ æ²¡æœ‰æ–°å¢ chunkï¼Œè·³è¿‡ç”»åƒæ›´æ–°ã€‚")
        return False

    old_profile = ""
    if os.path.exists(PROFILE_PATH):
        with open(PROFILE_PATH, "r", encoding="utf-8") as f:
            old_profile = f.read().strip()
            state = _load_state()
            state["last_line"] = new_last_line
            _save_state(state)


    # å‹ç¼© evidence
    evidence = []
    for c in chunks:
        text = (c.get("text") or "").strip().replace("\n", " ")
        text = text[:500]
        evidence.append(
            f"- source={c.get('source')} weight={c.get('weight')} file={c.get('file_path')} created_at={c.get('created_at')}\n"
            f"  text={text}"
        )
    evidence_block = "\n".join(evidence)

    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.2)

    system = (
        "ä½ æ˜¯â€œç”¨æˆ·ç”»åƒæ›´æ–°å™¨â€ã€‚ä½ çš„ä»»åŠ¡ï¼šæ ¹æ®æ–°å¢è¯æ®ï¼Œæ›´æ–° user_profile.mdã€‚\n"
        "è§„åˆ™ï¼š\n"
        "1) è¾“å‡ºå¿…é¡»æ˜¯ Markdownï¼ˆä¸æ˜¯ JSONï¼‰ã€‚\n"
        "2) å°½é‡ä¿æŒç¨³å®šï¼Œåªåšå¢é‡æ›´æ–°ï¼Œä¸è¦å› ä¸ºå°‘é‡è¯æ®æ¨ç¿»æ—§ç»“è®ºã€‚\n"
        "3) ä»»ä½•æ–°å¢ç»“è®ºéƒ½è¦åœ¨â€œè¯æ®æ—¥å¿—â€é‡Œå†™æ˜æ¥æºï¼ˆsource/file/created_atï¼‰ã€‚\n"
        "4) æ–‡é£ï¼šç®€æ´ã€åƒå¤‡å¿˜å½•ã€‚\n"
        "è¯·ä½¿ç”¨å›ºå®šç»“æ„ï¼š\n"
        "# æ ¸å¿ƒæ€§æ ¼ä¸åå¥½\n"
        "# å†³ç­–ä¸å­¦ä¹ é£æ ¼\n"
        "# äº¤æ˜“é£æ ¼ä¸é£é™©åå¥½\n"
        "# å¸¸è§ç›²ç‚¹ä¸çº åæé†’\n"
        "# è¿‘æœŸå…³æ³¨ä¸å‡è®¾ï¼ˆå¯å˜åŒ–ï¼‰\n"
        "# è¯æ®æ—¥å¿—ï¼ˆè‡ªåŠ¨è¿½åŠ ï¼‰\n"
    )

    user = (
        f"ã€æ—§ç”»åƒã€‘\n{old_profile if old_profile else '(ç©º)'}\n\n"
        f"ã€æ–°å¢è¯æ®ï¼ˆæœ¬æ¬¡æ–°å¢ {raw_new_line_count} è¡Œ corpusï¼‰ã€‘\n{evidence_block}\n\n"
        "è¯·è¾“å‡ºæ›´æ–°åçš„å®Œæ•´ user_profile.md å†…å®¹ã€‚"
    )
    prompt = f"{system}\n\n{user}"
    resp = _retry(lambda: llm.invoke(prompt))
    new_profile = (resp.content or "").strip()

    if not new_profile:
        print("âŒ æ¨¡å‹è¾“å‡ºä¸ºç©ºï¼Œè·³è¿‡å†™å…¥ã€‚")
        return False

    with open(PROFILE_PATH, "w", encoding="utf-8") as f:
        f.write(new_profile + "\n")

    print(f"âœ… user_profile.md å·²æ›´æ–°ï¼ˆå¸æ”¶ {len(chunks)} æ¡é«˜æƒé‡è¯æ®ï¼‰")
    return True

if __name__ == "__main__":
    update_user_profile()
